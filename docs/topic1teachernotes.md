# Daydreaming and lazy thinking: Teachers' notes

**AW**: In some of these papers, the error bars on the Figures are wrong, because they calculated for each condition separately, while the analysis is obviously conducted on the _differences_. I've marked them as _bad graph_, below. You'll need to make sure students do not replicate this error. In some cases, there isn't a graph, but the same error has been made in a table. I've also given these the same label.

**AW**: Another issue with some papers is that the sample size is quite small (~10 participants). This isn't enough to get a good estimate of effect size, so it's possible that these studies won't replicate. I've marked them as _small sample_ below. It would be worth checking whether anyone has successfully replicated these studies with a larger sample.

**AW**: A third issue with some papers is that they use a median split. This is [bad practice](http://www.psychology.sunysb.edu/attachment/measures/content/maccallum_on_dichotomizing.pdf), so we should ensure students don't end up reproducing it. I've marked this as _dicohotomizing_, below.

**AW**: To estimate effects size for a within-subjects t-test, you need the
unstandardized difference score, and the standard deviation of the
differences. For example, in Exp. 3 of Pennycook et al. (2012), stereotype
consistent vs. congruent, the difference is 2.2 (see Table 2). The t = 2.46 and
t = diff/SE so SE = diff/t = .89. Note that means that they have either
incorrectly reported their either their t value, difference, or SE, because
they report the SE to be .021. If the SE were really .021. SE = SD/sqrt(N),
thus SD = SE * sqrt(N). So SD = .021 * sqrt(23) = .1. That would lead to a d of
about 20 if the difference is 2, which is extremely unlikely. Let's assume the
SE report is wrong (see below for reasoning). The df = 22, and hence N = 23. So
SD = .89*sqrt(23) = 4.27.  So d = 2.2/4.27 = .51.


## 1. When does our mind wander?

**CW**: Although there are many papers on this topic, I've selected the ones that focus on the main effect and have a decent effect size. 

**AW**: Seems like a good topic. I guess my main question would be, "how do you see students following this up?".

### Forster & Lavie (2009) (_small sample_, _bad graph_)

**CW**: Experiments 1A and 1B are key. Effect size is d = 1.60 for Experiment 1A, and d = 1.20 for Experiment 1B.

**AW**: The trial-unique random generation of stimuli required is possible in Open Sesame, but you'll need to think how you support students to use a design like this (it's a bit beyond introductory level). 

**CW** I have replaced this paper with one which has a larger sample and a simpler design

### Xu & Metcalfe (2016) (_bad graph_)

**CW**: Experiment 2 is key; effect size is: partial-eta-squared = .27

**CW**: Students would need to develop stimuli at range of difficulties, and would need to reduce testing time. 

**AW**: Agreed.

### Teasdale et al. (1993) (_small sample_, _bad graph_)

**CW**: Focus on Experiments 1 and 3.

**AW**: I'd say students could focus on only Experiment 1 with losing much here. The only thing Exp. 3 adds to Exp. 1 is an inconclusive manipulation of load size, right?

**AW**: Students would need to simplify the design for the purposes of this module. Experiment 1 has two w/subj factors (speed and load). They'd need to pick just one. And, if they pick load, it would need to be at the slow speed (because at fast, the load manipulation doesn't work very well). 

**CW**: Experiment 1: Estimated partial-eta-squared for the working memory load is about .5 (.55 for Experiment 1, .52 for Experiment 3). For presentation rate, it's .3.

**AW**: If keeping with auditory stimuli, you'll need to ensure that students have sufficient pairs of headphones available to them for testing -- probably worth getting these ordered through the tech office now? And I'd recommend moving from verbal to keyboard responding, as verbal responding isn't going to be easy to achieve or code in the mass-testing arrangements of this module. This means participants won't be able to keep their eyes closed ... does that matter? Students may also want to think about whether they really want people to make a free-text responding to the though probes. It might be better to switch to the technique used in some other papers where participants just report whether their thought was task-related or task-unrelated?

**CW**: I have replaced this paper with one which has a simpler design. Study 2 using tapping so no verbalisation required for that one. Probes and responses can be done on a computer with eyes open as is typically done now.


## 2. Where does our mind wander?

**AW**: Seems like a good topic. I guess my main question would be, "how do you see students following this up?".

### Baird et al. (2011) (_bad graph_)

**AW**: Students would have to simplify their analysis relative to what is reported here, because the paper involves an interaction, and for this module it needs to be a single-factor design. The obvious thing to do is to only analyze the off-task thoughts. The effect size for off-task thoughts here is sufficient, partial-eta-squared = .36

**AW**: And, presumably, we'd ask students to avoid the OSPAN part of the task, as that involves analysis techniques from PSYC519.

### Stawarczyk et al. (2011) (_bad graph_)

**CW**: Focus on Experiment 2, ignore the b/subj manipulation. Effect size is, partial-eta-squared = .27 for temporal orientation -- future more than others.

**AW**: Looks to me like that effect size mainly comes from the personal goals condition. This implies that students need to follow up that condition, rather than the control condition, to stand the best chance of finding something.

### Seli et al. (2017) (_bad graph_)

**CW**: Past vs. future effect size is partial-eta-squared = 0.25.

**AW**: Seems like a good study.

## 3. Are we really lazy thinkers?

**AW**: I think that, in the first two papers, there's an interesting sub-topic ... but that there are problems with the specific experiments you've focussed on. Perhaps there are other, more suitable, papers on the same topic? The De Neys et al. (2013) is an odd-one-out, and also doesn't look that well suited to the format of this module. 

### Seli et al. (2017) (_small sample_, _bad graph_, _dicohotomizing_)

**CW**: People tend to neglect base rate information when making judgments, but they recall that base rate information better if it was incongruent with stereotype than congruent with it. Partial eta-squared is .46

**AW**: The above refers to Experiment 1, but I don't think you can do Experiment 1 in this module. It uses a "thinking aloud" procedure, which I doubt is going to be practical in our mass testing arrangements. Might Experiment 2 be more suitable? 

### Pennycook et al. (2012) _UNDERPOWERED_

**CW**: People are slower to give the stereotypical response when it conflicts with base rates than when it doesn’tFocus on experiments 3 and 4, as these are the conditions under which the effect occurs. The effect size isn't reported, and I'm unsure how to calculate it.

**AW**: Short answer is that experiments based on this procedure will likely be underpowered in the context of our class. The effect size is about 0.5. I'm also a bit concerned that they test 32 people but can only analyse 23, a 28% dropout rate that would leave our students with an effective N of around 18. For N=18, it's d>=.61. So overall, this effect is really a bit too small for this class.

#### De Neys et al. (2013)

**CW**: They are slower to give the stereotypical response when it conflicts with base rates than when it doesn’t. Effect size is partial eta-square = .23, which is slightly low, but it's a nice, accessible paper.

**AW**: I agree it's a nicely accessible study, and I think the effect size we
can live with, particularly as there's a successful replication on [OSF](https://osf.io/79urz/#!). A bigger issue is that the procedure is extraordinarily short - two short problems, where they give their answer and a confidence rating. I
don't think it's a sufficiently challenging task in terms of the goal of
getting them to set up an experiment in Open Sesame. It's also not a great fit
in terms of testing time against all the other topics and sub-topics - I assume
the whole thing takes less than 5 minutes per participant. Another issue is that it doesn't group particularly well with the other two papers in this topic. For your other two topics, all papers are about the same issue. Here, two papers are and one is not.
